{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "General Cleaning"
      ],
      "metadata": {
        "id": "kXar3t41UJ1f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "STvkSSW7Sm4d",
        "outputId": "5ec0d41a-8286-4728-cd91-0e084a18351f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ANnumm7ydyjd"
      },
      "outputs": [],
      "source": [
        "folder_path = '/content/drive/My Drive/FYP/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7wTOXQBvd1JI"
      },
      "outputs": [],
      "source": [
        "# Function to clean individual crop data\n",
        "import pandas as pd\n",
        "\n",
        "def clean_crop_data(file_path):\n",
        "    # Load the dataset\n",
        "    crop_data = pd.read_csv(file_path)\n",
        "\n",
        "    # Step 1: Convert 'Date' to proper datetime format\n",
        "    crop_data['Date'] = pd.to_datetime(crop_data['Date'], format='%d-%B-%Y', errors='coerce')\n",
        "\n",
        "    # Step 2: Handle missing values in 'textbox2' (crop prices)\n",
        "    crop_data['textbox2'] = crop_data['textbox2'].replace('NA', None)\n",
        "    crop_data['textbox2'] = pd.to_numeric(crop_data['textbox2'].str.replace(',', ''), errors='coerce')\n",
        "\n",
        "    # Step 3: Rename columns for clarity\n",
        "    crop_data = crop_data.rename(columns={\n",
        "        'textbox3': 'Location',\n",
        "        'textbox2': 'Price',\n",
        "        'CropNameUrdu': 'CropName'\n",
        "    })\n",
        "\n",
        "    # Step 4: Keep only the first word in the 'Location' column\n",
        "    crop_data['Location'] = crop_data['Location'].str.split().str[0]\n",
        "\n",
        "    # Step 5: Drop unnecessary columns ('Month' and 'Year')\n",
        "    crop_data_cleaned = crop_data.drop(columns=['Month', 'Year'])\n",
        "\n",
        "    # Step 6: Impute missing prices using interpolation\n",
        "    crop_data_cleaned['Price'] = crop_data_cleaned['Price'].interpolate(method='linear', limit_direction='forward')\n",
        "\n",
        "    return crop_data_cleaned"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pre-Processing"
      ],
      "metadata": {
        "id": "KwU6vR9vUoQ6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vEXfugncd4Pe"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import os\n",
        "\n",
        "# Function to load and preprocess data\n",
        "def load_and_prepare_data(file_path):\n",
        "    data = pd.read_csv(file_path)\n",
        "\n",
        "    # Handle any necessary cleaning (e.g., missing values)\n",
        "    data['Price'].replace({'NA': None})\n",
        "    data.dropna(subset=['Price'], inplace=True)\n",
        "    # Encode categorical columns\n",
        "    label_encoder_location = LabelEncoder()\n",
        "    label_encoder_crop = LabelEncoder()\n",
        "\n",
        "    data['Location_encoded'] = label_encoder_location.fit_transform(data['Location'])\n",
        "    data['CropName_encoded'] = label_encoder_crop.fit_transform(data['CropName'])\n",
        "\n",
        "    # Normalize prices (LSTM works better with normalized data)\n",
        "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "    data['Price'] = scaler.fit_transform(data['Price'].values.reshape(-1, 1))\n",
        "\n",
        "    # Convert date column to datetime format and extract useful features\n",
        "    data['Date'] = pd.to_datetime(data['Date'])\n",
        "    data['Day'] = data['Date'].dt.day\n",
        "    data['Month'] = data['Date'].dt.month\n",
        "    data['Year'] = data['Date'].dt.year\n",
        "\n",
        "    return data, label_encoder_location, label_encoder_crop, scaler\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUZVlgtI6e-T"
      },
      "source": [
        "Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OIoqjOpv6eH1"
      },
      "outputs": [],
      "source": [
        "# Load and concatenate multiple years of data\n",
        "folder_path = '/content/drive/My Drive/FYP/'\n",
        "all_data = pd.DataFrame()\n",
        "\n",
        "for year in range(2007, 2025):\n",
        "    file_path = f'{folder_path}/cleaned_{year}.csv'\n",
        "    year_data, location_encoder, crop_encoder, scaler = load_and_prepare_data(file_path)\n",
        "    all_data = pd.concat([all_data, year_data])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Debugging"
      ],
      "metadata": {
        "id": "FB6Rzug4UT1Q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IgjyfM_MhhY5"
      },
      "outputs": [],
      "source": [
        "num_location = len(location_encoder.classes_)  # Should match the max value in Location_encoded\n",
        "num_crop = len(crop_encoder.classes_)  # Should match the max value in CropName_encoded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q11AaCvAhlSk",
        "outputId": "e0700cac-9e8e-4933-a62c-064988b14f73"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "137 138\n",
            "131 132\n"
          ]
        }
      ],
      "source": [
        "# Check for any out-of-range indices\n",
        "print(all_data['Location_encoded'].max(), len(location_encoder.classes_))\n",
        "print(all_data['CropName_encoded'].max(), len(crop_encoder.classes_))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C640EEsghyli"
      },
      "outputs": [],
      "source": [
        "# Check that all unique locations and crops are encoded properly\n",
        "location_mapping = dict(zip(location_encoder.classes_, range(len(location_encoder.classes_))))\n",
        "crop_mapping = dict(zip(crop_encoder.classes_, range(len(crop_encoder.classes_))))\n",
        "\n",
        "# Apply encoding (if it wasn't already done correctly)\n",
        "all_data['Location_encoded'] = all_data['Location'].map(location_mapping)\n",
        "all_data['CropName_encoded'] = all_data['CropName'].map(crop_mapping)\n",
        "\n",
        "# Ensure no missing mappings\n",
        "assert all_data['Location_encoded'].notna().all(), \"Some locations were not mapped correctly\"\n",
        "assert all_data['CropName_encoded'].notna().all(), \"Some crops were not mapped correctly\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "StDwX0zah_Fu",
        "outputId": "e1e99cb2-584a-430e-c121-5cfa77fae319"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Problematic Locations:\n",
            " Empty DataFrame\n",
            "Columns: [Location, Date, CropName, Price, Location_encoded, CropName_encoded, Day, Month, Year]\n",
            "Index: []\n",
            "Problematic Crops:\n",
            " Empty DataFrame\n",
            "Columns: [Location, Date, CropName, Price, Location_encoded, CropName_encoded, Day, Month, Year]\n",
            "Index: []\n"
          ]
        }
      ],
      "source": [
        "# Find rows with problematic Location/Crop encoding\n",
        "problematic_location_rows = all_data[all_data['Location_encoded'] >= num_location]\n",
        "problematic_crop_rows = all_data[all_data['CropName_encoded'] >= num_crop]\n",
        "\n",
        "print(\"Problematic Locations:\\n\", problematic_location_rows)\n",
        "print(\"Problematic Crops:\\n\", problematic_crop_rows)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "is_2M0o7iZKM",
        "outputId": "4086e584-7785-4f17-ae7f-b5e9c369e800"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking for NaN values in Location_encoded and CropName_encoded...\n",
            "0 NaN values found in Location_encoded\n",
            "0 NaN values found in CropName_encoded\n"
          ]
        }
      ],
      "source": [
        "print(\"Checking for NaN values in Location_encoded and CropName_encoded...\")\n",
        "print(all_data['Location_encoded'].isna().sum(), \"NaN values found in Location_encoded\")\n",
        "print(all_data['CropName_encoded'].isna().sum(), \"NaN values found in CropName_encoded\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gvMb4JLZib6A",
        "outputId": "ff9403d5-49b7-4a60-e668-07999a7b9754"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rows with problematic Location encoding:\n",
            "Empty DataFrame\n",
            "Columns: [Location, Date, CropName, Price, Location_encoded, CropName_encoded, Day, Month, Year]\n",
            "Index: []\n",
            "Rows with problematic CropName encoding:\n",
            "Empty DataFrame\n",
            "Columns: [Location, Date, CropName, Price, Location_encoded, CropName_encoded, Day, Month, Year]\n",
            "Index: []\n"
          ]
        }
      ],
      "source": [
        "problematic_location_rows = all_data[all_data['Location_encoded'].isna()]\n",
        "problematic_crop_rows = all_data[all_data['CropName_encoded'].isna()]\n",
        "\n",
        "print(\"Rows with problematic Location encoding:\")\n",
        "print(problematic_location_rows)\n",
        "\n",
        "print(\"Rows with problematic CropName encoding:\")\n",
        "print(problematic_crop_rows)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Izy9BTCis5n",
        "outputId": "1c37f47f-0711-4784-db17-66e97c4c35c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Missing locations: set()\n",
            "Missing crops: set()\n"
          ]
        }
      ],
      "source": [
        "# Check if any locations/crops are missing from the encoder\n",
        "missing_locations = set(all_data['Location'].unique()) - set(location_encoder.classes_)\n",
        "missing_crops = set(all_data['CropName'].unique()) - set(crop_encoder.classes_)\n",
        "\n",
        "print(f\"Missing locations: {missing_locations}\")\n",
        "print(f\"Missing crops: {missing_crops}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Solution Debugging"
      ],
      "metadata": {
        "id": "F0ktNIoPUb72"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lxVbvXwsiw4D"
      },
      "outputs": [],
      "source": [
        "# Refit encoder to include all unique values\n",
        "location_encoder.fit(all_data['Location'].unique())\n",
        "crop_encoder.fit(all_data['CropName'].unique())\n",
        "\n",
        "# Apply the updated encoding\n",
        "all_data['Location_encoded'] = location_encoder.transform(all_data['Location'])\n",
        "all_data['CropName_encoded'] = crop_encoder.transform(all_data['CropName'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Converting to Tensors"
      ],
      "metadata": {
        "id": "CrOuocOFUeCo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SIKO8lzoNyFU",
        "outputId": "b1dc49a7-6f1d-4635-99c3-5b2a1154af4d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-07cf63d4f145>:54: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  sequences = torch.load(seq_file_path)\n",
            "<ipython-input-7-07cf63d4f145>:55: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  labels = torch.load(lbl_file_path)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data loaded from /content/drive/MyDrive/FYP/sequences.pt and /content/drive/MyDrive/FYP/labels.pt\n",
            "Using previously saved sequences and labels.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import os\n",
        "\n",
        "# File paths for saving/loading sequences and labels\n",
        "sequences_file = '/content/drive/MyDrive/FYP/sequences.pt'\n",
        "labels_file = '/content/drive/MyDrive/FYP/labels.pt'\n",
        "\n",
        "# Chunk-based sequence creation function\n",
        "def create_sequences_in_batches(data, n_steps=30, batch_size=100000):\n",
        "    sequences = []\n",
        "    labels = []\n",
        "    features = ['Location_encoded', 'CropName_encoded', 'Day', 'Month', 'Year', 'Price']\n",
        "\n",
        "    # Convert data to NumPy array for faster processing\n",
        "    data_values = data[features].values\n",
        "    price_values = data['Price'].values\n",
        "\n",
        "    total_rows = len(data) - n_steps\n",
        "\n",
        "    # Process data in chunks\n",
        "    for start in range(0, total_rows, batch_size):\n",
        "        end = min(start + batch_size, total_rows)\n",
        "        chunk_sequences = []\n",
        "        chunk_labels = []\n",
        "\n",
        "        for i in range(start, end):\n",
        "            seq_data = data_values[i:i + n_steps]  # Faster access using NumPy slicing\n",
        "            label = price_values[i + n_steps]\n",
        "\n",
        "            chunk_sequences.append(seq_data)\n",
        "            chunk_labels.append(label)\n",
        "        np.array(chunk_sequences)\n",
        "        # Convert chunk to tensor and append to main list\n",
        "        sequences.append(torch.Tensor(np.array(chunk_sequences)))\n",
        "        labels.append(torch.Tensor(np.array(chunk_labels)))\n",
        "\n",
        "        print(f\"Processed rows from {start} to {end}\")\n",
        "\n",
        "    # Concatenate all batches\n",
        "    return torch.cat(sequences), torch.cat(labels)\n",
        "\n",
        "# Function to save the sequences and labels\n",
        "def save_data(sequences, labels, seq_file_path, lbl_file_path):\n",
        "    torch.save(sequences, seq_file_path)\n",
        "    torch.save(labels, lbl_file_path)\n",
        "    print(f\"Data saved to {seq_file_path} and {lbl_file_path}\")\n",
        "\n",
        "# Function to load the sequences and labels if they already exist\n",
        "def load_data(seq_file_path, lbl_file_path):\n",
        "    if os.path.exists(seq_file_path) and os.path.exists(lbl_file_path):\n",
        "        sequences = torch.load(seq_file_path)\n",
        "        labels = torch.load(lbl_file_path)\n",
        "        print(f\"Data loaded from {seq_file_path} and {lbl_file_path}\")\n",
        "        return sequences, labels\n",
        "    else:\n",
        "        return None, None\n",
        "\n",
        "# Check if the saved data exists\n",
        "sequences, labels = load_data(sequences_file, labels_file)\n",
        "\n",
        "if sequences is None or labels is None:\n",
        "    print(\"No saved data found. Processing sequences and labels...\")\n",
        "\n",
        "    # Split your large data into smaller chunks and create sequences\n",
        "    n_steps = 30\n",
        "    batch_size = 1000000  # Adjust based on memory\n",
        "    sequences, labels = create_sequences_in_batches(all_data, n_steps=n_steps, batch_size=batch_size)\n",
        "\n",
        "    # Save the processed sequences and labels to disk\n",
        "    save_data(sequences, labels, sequences_file, labels_file)\n",
        "else:\n",
        "    print(\"Using previously saved sequences and labels.\")\n",
        "\n",
        "# Create DataLoader\n",
        "dataset = TensorDataset(sequences, labels)\n",
        "data_loader = DataLoader(dataset, batch_size=64, shuffle=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Preparation"
      ],
      "metadata": {
        "id": "SWuYTgzMUlKd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UXmE3_y6NrLz"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class LSTMPricePredictor(nn.Module):\n",
        "    def __init__(self, num_location, num_crop, hidden_size, num_layers, output_size):\n",
        "        super(LSTMPricePredictor, self).__init__()\n",
        "\n",
        "        # Embeddings for Location and CropName\n",
        "        self.location_embedding = nn.Embedding(num_location, 10)  # Embedding size 10\n",
        "        self.crop_embedding = nn.Embedding(num_crop, 10)  # Embedding size 10\n",
        "\n",
        "        # LSTM\n",
        "        input_size = 24  # Updated input size: 10 (location) + 10 (crop) + 4 (other features)\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Embed the categorical features (location and crop)\n",
        "        location_embedded = self.location_embedding(x[:, :, 0].long())  # Shape: [batch_size, sequence_len, 10]\n",
        "        crop_embedded = self.crop_embedding(x[:, :, 1].long())  # Shape: [batch_size, sequence_len, 10]\n",
        "\n",
        "        # Extract the other features (Day, Month, Year, and Price)\n",
        "        other_features = x[:, :, 2:]  # Shape: [batch_size, sequence_len, 4]\n",
        "\n",
        "        # Concatenate embeddings with other features along the feature dimension (dim=2)\n",
        "        x = torch.cat((location_embedded, crop_embedded, other_features), dim=2)\n",
        "\n",
        "        # LSTM forward pass\n",
        "        h0 = torch.zeros(2, x.size(0), 128).to(x.device)  # Initial hidden state\n",
        "        c0 = torch.zeros(2, x.size(0), 128).to(x.device)  # Initial cell state\n",
        "        out, _ = self.lstm(x, (h0, c0))\n",
        "\n",
        "        # Pass the output of the last time step to the fully connected layer\n",
        "        out = self.fc(out[:, -1, :])  # Shape: [batch_size, output_size]\n",
        "        return out\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the model\n",
        "hidden_size = 128\n",
        "num_layers = 2\n",
        "output_size = 1  # Predicting price\n",
        "\n",
        "model = LSTMPricePredictor(num_location=len(location_encoder.classes_),\n",
        "                           num_crop=len(crop_encoder.classes_),\n",
        "                           hidden_size=hidden_size,\n",
        "                           num_layers=num_layers,\n",
        "                           output_size=output_size)\n",
        "\n",
        "# Loss and optimizer\n",
        "loss_fn = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
        "\n",
        "# Move to device (GPU if available)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "FYz3G0jX474Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Training"
      ],
      "metadata": {
        "id": "p-Wytcs-UqCw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DsoEG_ypvPFF",
        "outputId": "b21bf41e-6356-4432-feb5-999bf0b3f530"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [1/3]: 100%|██████████| 370677/370677 [33:20<00:00, 185.31it/s, Loss=0.00302]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/3], Loss: 0.0030\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [2/3]: 100%|██████████| 370677/370677 [33:20<00:00, 185.33it/s, Loss=0.00217]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [2/3], Loss: 0.0022\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch [3/3]: 100%|██████████| 370677/370677 [33:14<00:00, 185.85it/s, Loss=0.00191]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [3/3], Loss: 0.0019\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "def train_model(model, data_loader, num_epochs):\n",
        "    model.train()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0\n",
        "\n",
        "        # Create a progress bar for the current epoch\n",
        "        progress_bar = tqdm(data_loader, desc=f\"Epoch [{epoch+1}/{num_epochs}]\")\n",
        "\n",
        "        for sequences, labels in progress_bar:\n",
        "            sequences, labels = sequences.to(device), labels.to(device)\n",
        "            sequences = sequences.float()  # Keep sequences as float, but no unsqueeze\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(sequences)\n",
        "            loss = loss_fn(outputs.squeeze(), labels)\n",
        "\n",
        "            # Backward pass and optimization\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Update the progress bar with the current loss\n",
        "            progress_bar.set_postfix({'Loss': total_loss / len(data_loader)})\n",
        "\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss/len(data_loader):.4f}')\n",
        "\n",
        "        # Save the model after each epoch\n",
        "        model_path = f'/content/drive/My Drive/FYP/lstm_price_model_epoch_{epoch+1}.pth'\n",
        "        torch.save(model.state_dict(), model_path)\n",
        "\n",
        "# Train for a few epochs and save progress\n",
        "train_model(model, data_loader, num_epochs=3)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import os\n",
        "# Load the saved model state\n",
        "model_path = '/content/drive/My Drive/FYP/lstm_price_model_epoch_3.pth'\n",
        "model.load_state_dict(torch.load(model_path))\n",
        "model.eval()  # Set the model to evaluation mode\n",
        "\n",
        "# Function to prepare input data for prediction\n",
        "def prepare_input(location, date, crop_name):\n",
        "    # Encode the location and crop name\n",
        "    location_encoded = location_encoder.transform([location])[0]\n",
        "    crop_encoded = crop_encoder.transform([crop_name])[0]\n",
        "\n",
        "    # Extract date features\n",
        "    date = pd.to_datetime(date)\n",
        "    day = date.day\n",
        "    month = date.month\n",
        "    year = date.year\n",
        "\n",
        "    # Create a sequence with the same values repeated (since we need a sequence of length n_steps)\n",
        "    sequence = np.array([[location_encoded, crop_encoded, day, month, year, 0]] * 30)  # Price is set to 0 for prediction\n",
        "\n",
        "    # Convert to tensor\n",
        "    sequence_tensor = torch.Tensor(sequence).unsqueeze(0).to(device)  # Add batch dimension and move to device\n",
        "\n",
        "    return sequence_tensor\n",
        "\n",
        "# Function to predict price in rupees\n",
        "def predict_price_in_rupees(location, date, crop_name):\n",
        "    input_tensor = prepare_input(location, date, crop_name)\n",
        "    with torch.no_grad():\n",
        "        predicted_price_normalized = model(input_tensor).item()\n",
        "\n",
        "    # Inverse transform to get the price in rupees\n",
        "    predicted_price_rupees = scaler.inverse_transform([[predicted_price_normalized]])[0][0]\n",
        "    return predicted_price_rupees\n",
        "\n",
        "# Usage\n",
        "location = 'AhmadPurEast'\n",
        "date = '2021-04-01'\n",
        "crop_name = 'Melon'\n",
        "\n",
        "predicted_price_rupees = predict_price_in_rupees(location, date, crop_name)\n",
        "print(f'Predicted price for {crop_name} in {location} on {date}: {predicted_price_rupees} rupees')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "amiPMeLU4FQK",
        "outputId": "c84c7559-25c2-4c24-917e-1ace78acdc69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted price for Melon in AhmadPurEast on 2021-04-01: 3603.8787317946553 rupees\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-28-9da408fcc7d0>:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(model_path))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve the actual price from all_data for comparison\n",
        "actual_price_row = all_data[(all_data['Location'] == location) &\n",
        "                            (all_data['CropName'] == crop_name) &\n",
        "                            (all_data['Date'] == date)]\n",
        "\n",
        "if not actual_price_row.empty:\n",
        "    actual_price = actual_price_row['Price'].values[0]\n",
        "    actual_price_rupees = scaler.inverse_transform([[actual_price]])[0][0]\n",
        "    print(f'Actual price for {crop_name} in {location} on {date}: {actual_price_rupees} rupees')\n",
        "else:\n",
        "    print(f'No actual price data available for {crop_name} in {location} on {date}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sXbzXNBm56xi",
        "outputId": "c4b929e1-bd1b-40bd-c335-1981029b6a41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Actual price for Melon in AhmadPurEast on 2021-04-01: 3391.442061602206 rupees\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_Zv1WXJ6G6L"
      },
      "source": [
        "...."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
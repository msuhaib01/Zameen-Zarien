{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "STvkSSW7Sm4d",
        "outputId": "73fd4a96-be6b-4f44-a853-a45786bff867"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "folder_path = '/content/drive/My Drive/FYP/'"
      ],
      "metadata": {
        "id": "ANnumm7ydyjd"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to clean individual crop data\n",
        "import pandas as pd\n",
        "\n",
        "def clean_crop_data(file_path):\n",
        "    # Load the dataset\n",
        "    crop_data = pd.read_csv(file_path)\n",
        "\n",
        "    # Step 1: Convert 'Date' to proper datetime format\n",
        "    crop_data['Date'] = pd.to_datetime(crop_data['Date'], format='%d-%B-%Y', errors='coerce')\n",
        "\n",
        "    # Step 2: Handle missing values in 'textbox2' (crop prices)\n",
        "    crop_data['textbox2'] = crop_data['textbox2'].replace('NA', None)\n",
        "    crop_data['textbox2'] = pd.to_numeric(crop_data['textbox2'].str.replace(',', ''), errors='coerce')\n",
        "\n",
        "    # Step 3: Rename columns for clarity\n",
        "    crop_data = crop_data.rename(columns={\n",
        "        'textbox3': 'Location',\n",
        "        'textbox2': 'Price',\n",
        "        'CropNameUrdu': 'CropName'\n",
        "    })\n",
        "\n",
        "    # Step 4: Keep only the first word in the 'Location' column\n",
        "    crop_data['Location'] = crop_data['Location'].str.split().str[0]\n",
        "\n",
        "    # Step 5: Drop unnecessary columns ('Month' and 'Year')\n",
        "    crop_data_cleaned = crop_data.drop(columns=['Month', 'Year'])\n",
        "\n",
        "    # Step 6: Impute missing prices using interpolation\n",
        "    crop_data_cleaned['Price'] = crop_data_cleaned['Price'].interpolate(method='linear', limit_direction='forward')\n",
        "\n",
        "    return crop_data_cleaned"
      ],
      "metadata": {
        "id": "7wTOXQBvd1JI"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import os\n",
        "\n",
        "# Function to load and preprocess data\n",
        "def load_and_prepare_data(file_path):\n",
        "    data = pd.read_csv(file_path)\n",
        "\n",
        "    # Handle any necessary cleaning (e.g., missing values)\n",
        "    data['Price'].replace({'NA': None})\n",
        "    data.dropna(subset=['Price'], inplace=True)\n",
        "    # Encode categorical columns\n",
        "    label_encoder_location = LabelEncoder()\n",
        "    label_encoder_crop = LabelEncoder()\n",
        "\n",
        "    data['Location_encoded'] = label_encoder_location.fit_transform(data['Location'])\n",
        "    data['CropName_encoded'] = label_encoder_crop.fit_transform(data['CropName'])\n",
        "\n",
        "    # Normalize prices (LSTM works better with normalized data)\n",
        "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "    data['Price'] = scaler.fit_transform(data['Price'].values.reshape(-1, 1))\n",
        "\n",
        "    # Convert date column to datetime format and extract useful features\n",
        "    data['Date'] = pd.to_datetime(data['Date'])\n",
        "    data['Day'] = data['Date'].dt.day\n",
        "    data['Month'] = data['Date'].dt.month\n",
        "    data['Year'] = data['Date'].dt.year\n",
        "\n",
        "    return data, label_encoder_location, label_encoder_crop, scaler\n"
      ],
      "metadata": {
        "id": "vEXfugncd4Pe"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Resume**"
      ],
      "metadata": {
        "id": "WUZVlgtI6e-T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and concatenate multiple years of data\n",
        "folder_path = '/content/drive/My Drive/FYP/'\n",
        "all_data = pd.DataFrame()\n",
        "\n",
        "for year in range(2007, 2025):\n",
        "    file_path = f'{folder_path}/cleaned_{year}.csv'\n",
        "    year_data, location_encoder, crop_encoder, scaler = load_and_prepare_data(file_path)\n",
        "    all_data = pd.concat([all_data, year_data])\n"
      ],
      "metadata": {
        "id": "OIoqjOpv6eH1"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_location = len(location_encoder.classes_)  # Should match the max value in Location_encoded\n",
        "num_crop = len(crop_encoder.classes_)  # Should match the max value in CropName_encoded\n"
      ],
      "metadata": {
        "id": "IgjyfM_MhhY5"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for any out-of-range indices\n",
        "print(all_data['Location_encoded'].max(), len(location_encoder.classes_))\n",
        "print(all_data['CropName_encoded'].max(), len(crop_encoder.classes_))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q11AaCvAhlSk",
        "outputId": "53325fb9-5395-41b6-d88c-cf82f2ecffd2"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "137 138\n",
            "131 132\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check that all unique locations and crops are encoded properly\n",
        "location_mapping = dict(zip(location_encoder.classes_, range(len(location_encoder.classes_))))\n",
        "crop_mapping = dict(zip(crop_encoder.classes_, range(len(crop_encoder.classes_))))\n",
        "\n",
        "# Apply encoding (if it wasn't already done correctly)\n",
        "all_data['Location_encoded'] = all_data['Location'].map(location_mapping)\n",
        "all_data['CropName_encoded'] = all_data['CropName'].map(crop_mapping)\n",
        "\n",
        "# Ensure no missing mappings\n",
        "assert all_data['Location_encoded'].notna().all(), \"Some locations were not mapped correctly\"\n",
        "assert all_data['CropName_encoded'].notna().all(), \"Some crops were not mapped correctly\"\n"
      ],
      "metadata": {
        "id": "C640EEsghyli"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find rows with problematic Location/Crop encoding\n",
        "problematic_location_rows = all_data[all_data['Location_encoded'] >= num_location]\n",
        "problematic_crop_rows = all_data[all_data['CropName_encoded'] >= num_crop]\n",
        "\n",
        "print(\"Problematic Locations:\\n\", problematic_location_rows)\n",
        "print(\"Problematic Crops:\\n\", problematic_crop_rows)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "StDwX0zah_Fu",
        "outputId": "d5673043-da6c-4119-b2bb-21fc571b87be"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Problematic Locations:\n",
            " Empty DataFrame\n",
            "Columns: [Location, Date, CropName, Price, Location_encoded, CropName_encoded, Day, Month, Year]\n",
            "Index: []\n",
            "Problematic Crops:\n",
            " Empty DataFrame\n",
            "Columns: [Location, Date, CropName, Price, Location_encoded, CropName_encoded, Day, Month, Year]\n",
            "Index: []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Checking for NaN values in Location_encoded and CropName_encoded...\")\n",
        "print(all_data['Location_encoded'].isna().sum(), \"NaN values found in Location_encoded\")\n",
        "print(all_data['CropName_encoded'].isna().sum(), \"NaN values found in CropName_encoded\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "is_2M0o7iZKM",
        "outputId": "4c273493-ac1c-4e62-9130-fe816def84d7"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking for NaN values in Location_encoded and CropName_encoded...\n",
            "0 NaN values found in Location_encoded\n",
            "0 NaN values found in CropName_encoded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "problematic_location_rows = all_data[all_data['Location_encoded'].isna()]\n",
        "problematic_crop_rows = all_data[all_data['CropName_encoded'].isna()]\n",
        "\n",
        "print(\"Rows with problematic Location encoding:\")\n",
        "print(problematic_location_rows)\n",
        "\n",
        "print(\"Rows with problematic CropName encoding:\")\n",
        "print(problematic_crop_rows)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gvMb4JLZib6A",
        "outputId": "47a4148d-fc7f-4b3c-9ce9-7cd7d3215204"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rows with problematic Location encoding:\n",
            "Empty DataFrame\n",
            "Columns: [Location, Date, CropName, Price, Location_encoded, CropName_encoded, Day, Month, Year]\n",
            "Index: []\n",
            "Rows with problematic CropName encoding:\n",
            "Empty DataFrame\n",
            "Columns: [Location, Date, CropName, Price, Location_encoded, CropName_encoded, Day, Month, Year]\n",
            "Index: []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if any locations/crops are missing from the encoder\n",
        "missing_locations = set(all_data['Location'].unique()) - set(location_encoder.classes_)\n",
        "missing_crops = set(all_data['CropName'].unique()) - set(crop_encoder.classes_)\n",
        "\n",
        "print(f\"Missing locations: {missing_locations}\")\n",
        "print(f\"Missing crops: {missing_crops}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Izy9BTCis5n",
        "outputId": "55bc6868-8e6a-4021-fac6-07a203b90f24"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Missing locations: set()\n",
            "Missing crops: set()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Refit encoder to include all unique values\n",
        "location_encoder.fit(all_data['Location'].unique())\n",
        "crop_encoder.fit(all_data['CropName'].unique())\n",
        "\n",
        "# Apply the updated encoding\n",
        "all_data['Location_encoded'] = location_encoder.transform(all_data['Location'])\n",
        "all_data['CropName_encoded'] = crop_encoder.transform(all_data['CropName'])\n"
      ],
      "metadata": {
        "id": "lxVbvXwsiw4D"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import os\n",
        "\n",
        "# File paths for saving/loading sequences and labels\n",
        "sequences_file = '/content/drive/MyDrive/FYP (1)/sequences.pt'\n",
        "labels_file = '/content/drive/MyDrive/FYP (1)/labels.pt'\n",
        "\n",
        "# Chunk-based sequence creation function\n",
        "def create_sequences_in_batches(data, n_steps=30, batch_size=100000):\n",
        "    sequences = []\n",
        "    labels = []\n",
        "    features = ['Location_encoded', 'CropName_encoded', 'Day', 'Month', 'Year', 'Price']\n",
        "\n",
        "    # Convert data to NumPy array for faster processing\n",
        "    data_values = data[features].values\n",
        "    price_values = data['Price'].values\n",
        "\n",
        "    total_rows = len(data) - n_steps\n",
        "\n",
        "    # Process data in chunks\n",
        "    for start in range(0, total_rows, batch_size):\n",
        "        end = min(start + batch_size, total_rows)\n",
        "        chunk_sequences = []\n",
        "        chunk_labels = []\n",
        "\n",
        "        for i in range(start, end):\n",
        "            seq_data = data_values[i:i + n_steps]  # Faster access using NumPy slicing\n",
        "            label = price_values[i + n_steps]\n",
        "\n",
        "            chunk_sequences.append(seq_data)\n",
        "            chunk_labels.append(label)\n",
        "\n",
        "        # Convert chunk to tensor and append to main list\n",
        "        sequences.append(torch.Tensor(chunk_sequences))\n",
        "        labels.append(torch.Tensor(chunk_labels))\n",
        "\n",
        "        print(f\"Processed rows from {start} to {end}\")\n",
        "\n",
        "    # Concatenate all batches\n",
        "    return torch.cat(sequences), torch.cat(labels)\n",
        "\n",
        "# Function to save the sequences and labels\n",
        "def save_data(sequences, labels, seq_file_path, lbl_file_path):\n",
        "    torch.save(sequences, seq_file_path)\n",
        "    torch.save(labels, lbl_file_path)\n",
        "    print(f\"Data saved to {seq_file_path} and {lbl_file_path}\")\n",
        "\n",
        "# Function to load the sequences and labels if they already exist\n",
        "def load_data(seq_file_path, lbl_file_path):\n",
        "    if os.path.exists(seq_file_path) and os.path.exists(lbl_file_path):\n",
        "        sequences = torch.load(seq_file_path)\n",
        "        labels = torch.load(lbl_file_path)\n",
        "        print(f\"Data loaded from {seq_file_path} and {lbl_file_path}\")\n",
        "        return sequences, labels\n",
        "    else:\n",
        "        return None, None\n",
        "\n",
        "# Check if the saved data exists\n",
        "sequences, labels = load_data(sequences_file, labels_file)\n",
        "\n",
        "if sequences is None or labels is None:\n",
        "    print(\"No saved data found. Processing sequences and labels...\")\n",
        "\n",
        "    # Split your large data into smaller chunks and create sequences\n",
        "    n_steps = 30\n",
        "    batch_size = 1000000  # Adjust based on memory\n",
        "    sequences, labels = create_sequences_in_batches(all_data, n_steps=n_steps, batch_size=batch_size)\n",
        "\n",
        "    # Save the processed sequences and labels to disk\n",
        "    save_data(sequences, labels, sequences_file, labels_file)\n",
        "else:\n",
        "    print(\"Using previously saved sequences and labels.\")\n",
        "\n",
        "# Create DataLoader\n",
        "dataset = TensorDataset(sequences, labels)\n",
        "data_loader = DataLoader(dataset, batch_size=64, shuffle=True)\n"
      ],
      "metadata": {
        "id": "SIKO8lzoNyFU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8faa8897-41cf-403b-9ddc-5d8d12823b14"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No saved data found. Processing sequences and labels...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-25-8cd318dde7d0>:37: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
            "  sequences.append(torch.Tensor(chunk_sequences))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed rows from 0 to 1000000\n",
            "Processed rows from 1000000 to 2000000\n",
            "Processed rows from 2000000 to 3000000\n",
            "Processed rows from 3000000 to 4000000\n",
            "Processed rows from 4000000 to 5000000\n",
            "Processed rows from 5000000 to 6000000\n",
            "Processed rows from 6000000 to 7000000\n",
            "Processed rows from 7000000 to 8000000\n",
            "Processed rows from 8000000 to 9000000\n",
            "Processed rows from 9000000 to 10000000\n",
            "Processed rows from 10000000 to 11000000\n",
            "Processed rows from 11000000 to 12000000\n",
            "Processed rows from 12000000 to 13000000\n",
            "Processed rows from 13000000 to 14000000\n",
            "Processed rows from 14000000 to 15000000\n",
            "Processed rows from 15000000 to 16000000\n",
            "Processed rows from 16000000 to 17000000\n",
            "Processed rows from 17000000 to 18000000\n",
            "Processed rows from 18000000 to 19000000\n",
            "Processed rows from 19000000 to 20000000\n",
            "Processed rows from 20000000 to 21000000\n",
            "Processed rows from 21000000 to 22000000\n",
            "Processed rows from 22000000 to 23000000\n",
            "Processed rows from 23000000 to 23723320\n",
            "Data saved to /content/drive/MyDrive/FYP (1)/sequences.pt and /content/drive/MyDrive/FYP (1)/labels.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class LSTMPricePredictor(nn.Module):\n",
        "    def __init__(self, num_location, num_crop, hidden_size, num_layers, output_size):\n",
        "        super(LSTMPricePredictor, self).__init__()\n",
        "\n",
        "        # Embeddings for Location and CropName\n",
        "        self.location_embedding = nn.Embedding(num_location, 10)  # Embedding size 10\n",
        "        self.crop_embedding = nn.Embedding(num_crop, 10)  # Embedding size 10\n",
        "\n",
        "        # LSTM\n",
        "        input_size = 24  # Updated input size: 10 (location) + 10 (crop) + 4 (other features)\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Embed the categorical features (location and crop)\n",
        "        location_embedded = self.location_embedding(x[:, :, 0].long())  # Shape: [batch_size, sequence_len, 10]\n",
        "        crop_embedded = self.crop_embedding(x[:, :, 1].long())  # Shape: [batch_size, sequence_len, 10]\n",
        "\n",
        "        # Extract the other features (Day, Month, Year, and Price)\n",
        "        other_features = x[:, :, 2:]  # Shape: [batch_size, sequence_len, 4]\n",
        "\n",
        "        # Concatenate embeddings with other features along the feature dimension (dim=2)\n",
        "        x = torch.cat((location_embedded, crop_embedded, other_features), dim=2)\n",
        "\n",
        "        # LSTM forward pass\n",
        "        h0 = torch.zeros(2, x.size(0), 128).to(x.device)  # Initial hidden state\n",
        "        c0 = torch.zeros(2, x.size(0), 128).to(x.device)  # Initial cell state\n",
        "        out, _ = self.lstm(x, (h0, c0))\n",
        "\n",
        "        # Pass the output of the last time step to the fully connected layer\n",
        "        out = self.fc(out[:, -1, :])  # Shape: [batch_size, output_size]\n",
        "        return out\n",
        "\n",
        "# Initialize the model\n",
        "hidden_size = 128\n",
        "num_layers = 2\n",
        "output_size = 1  # Predicting price\n",
        "\n",
        "model = LSTMPricePredictor(num_location=len(location_encoder.classes_),\n",
        "                           num_crop=len(crop_encoder.classes_),\n",
        "                           hidden_size=hidden_size,\n",
        "                           num_layers=num_layers,\n",
        "                           output_size=output_size)\n",
        "\n",
        "# Loss and optimizer\n",
        "loss_fn = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Move to device (GPU if available)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)\n"
      ],
      "metadata": {
        "id": "UXmE3_y6NrLz"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, data_loader, num_epochs):\n",
        "    model.train()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0\n",
        "\n",
        "        for sequences, labels in data_loader:\n",
        "            sequences, labels = sequences.to(device), labels.to(device)\n",
        "            sequences = sequences.float()  # Keep sequences as float, but no unsqueeze\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(sequences)\n",
        "            loss = loss_fn(outputs.squeeze(), labels)\n",
        "\n",
        "            # Backward pass and optimization\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss/len(data_loader):.4f}')\n",
        "\n",
        "        # Save the model after each epoch\n",
        "        model_path = f'/content/drive/My Drive/FYP (1)/lstm_price_model_epoch_{epoch+1}.pth'\n",
        "        torch.save(model.state_dict(), model_path)\n",
        "\n",
        "# Train for a few epochs and save progress\n",
        "train_model(model, data_loader, num_epochs=3)\n"
      ],
      "metadata": {
        "id": "h2xbfSMGRCAj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_future(model, last_n_days_data, n_days_to_predict):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        input_seq = torch.Tensor(last_n_days_data).unsqueeze(0).to(device)\n",
        "\n",
        "        for _ in range(n_days_to_predict):\n",
        "            output = model(input_seq)\n",
        "            predictions.append(output.item())\n",
        "\n",
        "            # Add the prediction to the sequence and remove the oldest day\n",
        "            next_input = torch.cat((input_seq[:, 1:, :], output.unsqueeze(0).unsqueeze(0)), dim=1)\n",
        "            input_seq = next_input\n",
        "\n",
        "    return predictions"
      ],
      "metadata": {
        "id": "JvUqzNyARUok"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "...."
      ],
      "metadata": {
        "id": "B_Zv1WXJ6G6L"
      }
    }
  ]
}